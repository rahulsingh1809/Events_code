{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7c648b-7530-4f5d-a84a-2207ac31b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 'news_events_2025_07_07.00000.jsonl' with 25422 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00001.jsonl' with 25934 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00002.jsonl' with 25442 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00003.jsonl' with 25412 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00004.jsonl' with 25985 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00005.jsonl' with 26205 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00050.jsonl' with 25549 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00052.jsonl' with 26851 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00053.jsonl' with 25545 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00058.jsonl' with 25427 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00076.jsonl' with 26000 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00077.jsonl' with 26559 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00078.jsonl' with 25867 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00080.jsonl' with 25745 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00083.jsonl' with 25735 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00084.jsonl' with 25648 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00085.jsonl' with 26178 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00086.jsonl' with 26143 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00090.jsonl' with 26228 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00093.jsonl' with 25654 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00098.jsonl' with 25787 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00102.jsonl' with 25390 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00103.jsonl' with 25767 records.\n",
      "âœ… Loaded 'news_events_2025_07_07.00104.jsonl' with 26312 records.\n",
      "\n",
      "âœ… Combined 24 files with 620785 records.\n",
      "âœ… Flattened events: (620785, 4)\n",
      "âœ… Cleaned companies: (148630, 4)\n",
      "âœ… Cleaned events: (620785, 8)\n",
      "âœ… Processed 0 â€“ 50,000 rows for company1\n",
      "âœ… Processed 50,000 â€“ 100,000 rows for company1\n",
      "âœ… Processed 100,000 â€“ 150,000 rows for company1\n",
      "âœ… Processed 150,000 â€“ 200,000 rows for company1\n",
      "âœ… Processed 200,000 â€“ 250,000 rows for company1\n",
      "âœ… Processed 250,000 â€“ 300,000 rows for company1\n",
      "âœ… Processed 300,000 â€“ 350,000 rows for company1\n",
      "âœ… Processed 350,000 â€“ 400,000 rows for company1\n",
      "âœ… Processed 400,000 â€“ 450,000 rows for company1\n",
      "âœ… Processed 450,000 â€“ 500,000 rows for company1\n",
      "âœ… Processed 500,000 â€“ 550,000 rows for company1\n",
      "âœ… Processed 550,000 â€“ 600,000 rows for company1\n",
      "âœ… Processed 600,000 â€“ 620,785 rows for company1\n",
      "âœ… Processed 0 â€“ 50,000 rows for company2\n",
      "âœ… Processed 50,000 â€“ 100,000 rows for company2\n",
      "âœ… Processed 100,000 â€“ 150,000 rows for company2\n",
      "âœ… Processed 150,000 â€“ 200,000 rows for company2\n",
      "âœ… Processed 200,000 â€“ 250,000 rows for company2\n",
      "âœ… Processed 250,000 â€“ 300,000 rows for company2\n",
      "âœ… Processed 300,000 â€“ 350,000 rows for company2\n",
      "âœ… Processed 350,000 â€“ 400,000 rows for company2\n",
      "âœ… Processed 400,000 â€“ 450,000 rows for company2\n",
      "âœ… Processed 450,000 â€“ 500,000 rows for company2\n",
      "âœ… Processed 500,000 â€“ 550,000 rows for company2\n",
      "âœ… Processed 550,000 â€“ 600,000 rows for company2\n",
      "âœ… Processed 600,000 â€“ 620,785 rows for company2\n",
      "âœ… Final dataset: (620785, 14)\n",
      "ðŸ’¾ Saved CSV -> C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\final_dataset.csv\n",
      "ðŸ’¾ Saved Parquet -> C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\final_dataset.parquet\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part1.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part2.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part3.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part4.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part5.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part6.csv with 100000 rows\n",
      "âœ… Saved C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\\final_dataset_part7.csv with 20785 rows\n",
      "ðŸŽ‰ Splitting complete. Data ready for SQL.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_json_file(file_path, lines=False):\n",
    "    path = Path(file_path)\n",
    "    if not path.exists():\n",
    "        print(f\" Error: File not found -> {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_json(str(path), lines=lines)\n",
    "        print(f\" Loaded '{path.name}' with {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\" Error while reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_all_json_from_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    if not folder.exists():\n",
    "        print(f\" Error: Folder not found -> {folder}\")\n",
    "        return None\n",
    "\n",
    "    all_dfs = []\n",
    "    for file in folder.glob(\"*.json*\"):  # matches .json and .jsonl\n",
    "        is_jsonl = file.suffix == \".jsonl\"\n",
    "        df = load_json_file(file, lines=is_jsonl)\n",
    "        if df is not None:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"\\n Combined {len(all_dfs)} files with {len(final_df)} records.\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\" No valid JSON files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "# === Load your folder ===\n",
    "df = load_all_json_from_folder(\n",
    "    r\"C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\"\n",
    ")\n",
    "\n",
    "if df is None:\n",
    "    raise ValueError(\"No data loaded. Check your folder path.\")\n",
    "\n",
    "\n",
    "def safe_parse(row):\n",
    "    try:\n",
    "        if isinstance(row, str):\n",
    "            return ast.literal_eval(row)  # safely parse Python-style strings\n",
    "        elif isinstance(row, (list, dict)):\n",
    "            return row\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "parsed_data = df['data'].apply(safe_parse)\n",
    "\n",
    "records = []\n",
    "for item in parsed_data.dropna():\n",
    "    if isinstance(item, list):\n",
    "        records.extend(item)\n",
    "\n",
    "events_df = pd.DataFrame(records)\n",
    "print(\" Flattened events:\", events_df.shape)\n",
    "\n",
    "# Expand attributes\n",
    "if \"attributes\" in events_df.columns:\n",
    "    attr_df = pd.json_normalize(events_df['attributes'])\n",
    "    events_flat = pd.concat([events_df.drop(columns=['attributes']), attr_df], axis=1)\n",
    "else:\n",
    "    events_flat = events_df.copy()\n",
    "\n",
    "# Extract company relationships\n",
    "def extract_company_ids(rel):\n",
    "    try:\n",
    "        comp1 = rel.get(\"company1\", {}).get(\"data\", {}).get(\"id\")\n",
    "        comp2 = rel.get(\"company2\", {}).get(\"data\", {}).get(\"id\")\n",
    "        return pd.Series([comp1, comp2])\n",
    "    except:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "if \"relationships\" in events_flat.columns:\n",
    "    events_flat[[\"company1_id\", \"company2_id\"]] = events_flat[\"relationships\"].apply(extract_company_ids)\n",
    "    events_flat = events_flat.drop(columns=[\"relationships\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "included_records = []\n",
    "for row in df[\"included\"]:\n",
    "    try:\n",
    "        items = json.loads(row.replace(\"'\", \"\\\"\")) if isinstance(row, str) else row\n",
    "        included_records.extend(items)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "companies_flat = pd.json_normalize(included_records)\n",
    "companies_only = companies_flat[companies_flat[\"type\"] == \"company\"].copy()\n",
    "\n",
    "companies_clean = companies_only[[\n",
    "    \"id\", \"attributes.company_name\", \"attributes.domain\", \"attributes.ticker\"\n",
    "]].rename(columns={\n",
    "    \"id\": \"company_id\",\n",
    "    \"attributes.company_name\": \"company_name\",\n",
    "    \"attributes.domain\": \"domain\",\n",
    "    \"attributes.ticker\": \"ticker\"\n",
    "}).drop_duplicates(subset=[\"company_id\"])\n",
    "\n",
    "print(\" Cleaned companies:\", companies_clean.shape)\n",
    "\n",
    "\n",
    "events_clean = events_flat[[\n",
    "    \"id\", \"type\", \"summary\", \"category\", \"found_at\", \"confidence\",\n",
    "    \"company1_id\", \"company2_id\"\n",
    "]].rename(columns={\n",
    "    \"id\": \"event_id\",\n",
    "    \"type\": \"event_type\",\n",
    "    \"found_at\": \"event_date\"\n",
    "})\n",
    "\n",
    "print(\" Cleaned events:\", events_clean.shape)\n",
    "\n",
    "\n",
    "def chunked_merge(events_df, companies_df, left_key, suffix, chunk_size=50000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(events_df), chunk_size):\n",
    "        chunk = events_df.iloc[i:i+chunk_size]\n",
    "        merged = chunk.merge(\n",
    "            companies_df,\n",
    "            how=\"left\",\n",
    "            left_on=left_key,\n",
    "            right_on=\"company_id\"\n",
    "        ).rename(columns={\n",
    "            \"company_name\": f\"{suffix}_name\",\n",
    "            \"domain\": f\"{suffix}_domain\",\n",
    "            \"ticker\": f\"{suffix}_ticker\"\n",
    "        }).drop(columns=[\"company_id\"])\n",
    "        chunks.append(merged)\n",
    "        print(f\" Processed {i:,} â€“ {i+len(chunk):,} rows for {suffix}\")\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "events_with_company1 = chunked_merge(events_clean, companies_clean, \"company1_id\", \"company1\")\n",
    "events_with_companies = chunked_merge(events_with_company1, companies_clean, \"company2_id\", \"company2\")\n",
    "\n",
    "final_df = events_with_companies\n",
    "print(\" Final dataset:\", final_df.shape)\n",
    "\n",
    "\n",
    "output_csv = r\"C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\final_dataset.csv\"\n",
    "output_parquet = r\"C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\final_dataset.parquet\"\n",
    "\n",
    "final_df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "final_df.to_parquet(output_parquet, index=False)\n",
    "\n",
    "print(f\"ðŸ’¾ Saved CSV -> {output_csv}\")\n",
    "print(f\"ðŸ’¾ Saved Parquet -> {output_parquet}\")\n",
    "\n",
    "\n",
    "output_dir = r\"C:\\Users\\rahusingh\\OneDrive - Chegg Inc\\Desktop\\Sample_events_datasets\\split_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 100000  # adjust rows per chunk\n",
    "num_chunks = (len(final_df) // chunk_size) + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    chunk = final_df.iloc[start:end]\n",
    "    if not chunk.empty:\n",
    "        out_path = os.path.join(output_dir, f\"final_dataset_part{i+1}.csv\")\n",
    "        chunk.to_csv(out_path, index=False)\n",
    "        print(f\" Saved {out_path} with {len(chunk)} rows\")\n",
    "\n",
    "print(\"Splitting complete. Data ready for SQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4814da-2930-4687-b3ae-072ee58fbba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
